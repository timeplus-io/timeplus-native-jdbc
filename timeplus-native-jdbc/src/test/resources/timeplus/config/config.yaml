# NOTE: User and query level settings are set up in "users.yaml" file.
# If you have accidentally specified user-level settings here, server won't start.
# You can either move the settings to the right place inside "users.xml" file
# or add skip_check_for_incorrect_settings: 1 here.
logger:
    # Possible levels [1]:
    # - none (turns off logging)
    # - fatal
    # - critical
    # - error
    # - warning
    # - notice
    # - information
    # - debug
    # - trace
    # [1]: https://github.com/pocoproject/poco/blob/poco-1.9.4-release/Foundation/include/Poco/Logger.h#L105-L114
    level: information
    log: /var/log/timeplusd-server/timeplusd-server.log
    errorlog: /var/log/timeplusd-server/timeplusd-server.err.log
    # Rotation policy
    # See https://github.com/pocoproject/poco/blob/poco-1.9.4-release/Foundation/include/Poco/FileChannel.h#L54-L85
    size: 1000M
    count: 10
    # console: 1
    # Default behavior is autodetection (log to console if not daemon mode and is tty)

    # Per level overrides (legacy):
    # For example to suppress logging of the ConfigReloader you can use:
    # NOTE: levels.logger is reserved, see below.
    # levels:
    #     ConfigReloader: none

    # Per level overrides:
    # For example to suppress logging of the RBAC for default user you can use:
    # (But please note that the logger name maybe changed from version to version, even after minor upgrade)
    # levels:
    #     - logger:
    #         name: 'ContextAccess (default)'
    #         level: none
    #     - logger:
    #         name: 'DatabaseOrdinary (test)'
    #         level: none

# It is the name that will be shown in the timeplusd-client.
# By default, anything with "production" will be highlighted in red in query prompt.
display_name: timeplusd

node:
    # cluster id this node belongs to. Only nodes in the same cluster id
    # can form a cluster
    cluster_id: timeplusd_cluster_exp

    # Listen specified address.
    # Use :: (wildcard IPv6 address), if you want to accept connections both with IPv4 and IPv6 from everywhere.
    # Notes:
    # If you open connections from wildcard address, make sure that at least one of the following measures applied:
    # - server is protected by firewall and not accessible from untrusted networks;
    # - all users are restricted to subset of network addresses (see users.xml);
    # - all users have strong passwords, only secure (TLS) interfaces are accessible, or connections are only made via TLS interfaces.
    # - users without password have readonly access.
    # See also: https://www.shodan.io/search?query=timeplusd
    # listen_host: '::'

    # Default values - try only listen localhost on IPv4 and IPv6.
    # listen_host: '::1'
    # listen_host: 127.0.0.1

    # Same for hosts without support for IPv6:
    listen_host: 0.0.0.0

    # advertised_host is used for other peer nodes to connect it.
    # By default, it is set to network addressable fqdn / hostname, but in some scenarios,
    # the machine's fqdn / hostname are not correctly configured or not addressable from
    # other peer nodes, we will need manually configure it.
    # For example, if 2 nodes are in different AWS regions, the default EC2 hostname
    # may not be addressable in the 2 regions, we will need configure an EIP for
    # each of them to make them network reachable and set the `advertised_host`.
    # point to the EIP.
    # Another example is in an on-prem env, the hostname is not correctly configured
    # which has the by default `localhost` as its name but with other network
    # addressable IPs, in this scenario, we need point `advertised_host` to the IP.
    advertised_host:

    # node locality refers to the geo-location like aws regions, on-prem racks, data centers etc
    # used by data placement strategy.
    # It is in key=value format separated by comma. Current supported keys are `region, az, datacenter, rack`
    locality: region=us-east-1,datacenter=us-east-1a

    # node attributes is a list of arbitrary strings, separated by colon, describing node topology, node capability
    # For example, hdd:7200rpm or ssd, cpu:arm64 etc
    attrs:

    # Roles that the current node has
    # Supported roles : Metadata, Data, Ingest, Query
    # `Data` role contains both `Ingest and Query` roles. We can split `Data` role into them for better perf / scalability
    # `Learner` role acts like a data backup / replacement preparation
    roles:
        role:
            - Metadata
            - Data

    # Public protocols and ports
    http:
        port: 3218
        is_tls_port: false

    # Public native protocol port for timeplusd-client
    tcp:
        port: 8463
        is_tls_port: false

    # Query on this port is historical table query by default
    table_tcp:
        port: 7587
        is_tls_port: false

    table_http:
        port: 8123
        is_tls_port: false

    # Compatibility with PostgreSQL protocol.
    # timeplusd will pretend to be PostgreSQL for applications connecting to this port.
    postgresql:
        port: 5432
        is_tls_port: false

# Path to historical data directory, with trailing slash.
path: /var/lib/timeplusd/

# Path to temporary data for processing hard queries.
tmp_path: /var/lib/timeplusd/tmp/

# Same for hosts without support for IPv6:
# listen_host: 0.0.0.0

query_state_checkpoint:
    # Checkpoint storage type: nativelog or local_file_system ('auto' or empty means auto-detect,))
    storage_type : auto
    # Path to local state checkpoint
    path: /var/lib/timeplusd/checkpoint/
    # State checkpoint interval in seconds
    interval: 900
    # Checkpoint expired if not accessed after `ttl` seconds
    last_access_ttl: 604800 # 7 days
    # Periodical last access check interval
    last_access_check_interval: 7200
    # When unsubscribe a query, wait grace interval to delete its checkpoints
    delete_grace_interval: 60
    # When server teardown, last checkpoint flush timeout in seconds
    teardown_flush_timeout: 60
    # If any processor state larger than this size, mark it as heavy state
    # To avoid checkpointing heavy state at the same time
    heavy_state_size_threshold: 524288000 # 500 MB

    # Checkpoint Log related configs
    log_segment_size: 1073741824 # 1GB
    log_max_entry_size: 10485760 # 10 MB
    log_max_cached_entries: 0 # Disable cache now for checkpoint log
    log_flush_interval_entries: 1
    log_flush_bytes: 1
    log_retention_size: 1
    log_retention_ms: 7200000 # 2 hours
    log_min_size_to_keep: 1

metadata:
    # Only node with `metadata` role will start metadata server internally
    server:
        # This is internal listen host / tcp port for metadata
        # It could be different to public listen host
        listen_host: 0.0.0.0
        tcp_port: 8464
        is_port_tls: false
        reuse_socket_address: true
        listen_backlog: 128
        socket_send_buffer_size: 0
        socket_receive_buffer_size: 0

        # connection_max_idle_ms = 0 means no idle timeout management for connections
        connection_max_idle_ms: 120000
        idle_connection_check_interval_ms: 1000

        # Number of network socket server per IP / Port
        # The network thread reads requests from the network
        # and push the request to the request channel / queue
        # For metadata, use 1 network threads
        network_threads: 1

        # Worker threads which picks up the requests from the request queue
        # And do the real work
        worker_threads: 4

        # Max outstanding queued requests for this host/port
        # when this limit reaches, reject more requests like back pressure
        max_queued_requests: 16

        # interval to log socket server metrics
        metrics_log_interval_ms: 60000

    # All nodes in the cluster will run a meta client since each node
    # will register against with meta (leader) node
    client:
        # Socket related configurations
        max_connections_per_ip: 1
        max_inflight_requests: 10000
        socket_send_buffer_size: 0
        socket_receive_buffer_size: 0
        tcp_keep_alive_probe_sec: 10
        tcp_nodelay: true # Disable Nagle algo
        reconnect_backoff_ms: 500
        reconnect_backoff_max_ms: 6000
        reconnect_max_retries: 0
        dns_refresh_on_fail_interval_sec: 5
        dns_resolution_backoff_ms: 1000
        dns_resolution_backoff_max_ms: 8000
        dns_resolution_max_retries: 0

        # Number of threads to handle server response per client
        response_worker_threads: 4

        # Number of clients to process incoming / outgoing request / response
        # in parallel in a sharded way for all nodes in the cluster
        # Client pool size is usually not greater than the total nodes
        # in the cluster
        client_pool_size: 1

        # Maximum write buffer size in bytes for one peer node
        # When max write buffer size is full, further write to the socket will fail
        max_send_buffer_size: 10485760 # 10 MB

        # interval to log socket client metrics
        metrics_log_interval_ms: 60000

    raft:
        # \tick_interval_ms is the default resolution of the Raft timer for every tick
        tick_interval_ms: 100
        # \reproposal_timeout_ticks is the number of ticks before
        # reproposing a Raft command
        reproposal_timeout_ticks: 6
        # \heartbeat_ticks determine the number of ticks between each heartbeat
        heartbeat_ticks: 2
        # \election_ticks specifies the minimum number of Raft ticks before
        # holding an election. The actual election timeout per shard replica is
        # multiplied by a random factor of 1-2 to avoid ties.
        election_ticks: 4
        # \max_size_per_message specifies the maximum aggregate byte size of Raft
        # log entries that a leader will send to followers in a single AppendMessage
        max_size_per_message: 10485760 # 10 MB
        # \max_committed_size specifies the maximum aggregate byte size of the committed
        # log entries which a node will receive in a single Ready
        max_committed_size: 67108864 # 64 MB
        # \max_uncommitted_entries_size controls how large the uncommitted tail of the
        # Raft log can grow. The limit is meant to provide protection against unbounded
        # Raft log growth when the quorum is lost and entries stop begging committed but
        # continue to be proposed.
        max_uncommitted_entries_size: 134217728 # 128 MB
        # \max_inflight_messages specifies how many `inflight` AppendMessages a leader
        # will send to a given follower without hearing a response.
        max_inflight_messages: 128
        # \max_inflight_bytes specifies how maximum aggregated byte size of Raft log entries
        # that a leader will send to a given follower without hearing a response. As such,
        # it also bounds the amount of replication data buffered in memory on the receiver.
        # Individual message can still exceed this limit (a big log entry for example)
        # Per the bandwidth-delay product, this will limit per shard throughput to
        # 256 MB/s at 500 ms RTT, ~1.28 GB /s at 100 ms RTT, ~12.8 GB/s at 10ms RTT
        max_inflight_bytes: 134217728 # 128 MB
        # \async_storage_writes shall be always true
        async_storage_writes: true
        check_quorum: true
        pre_vote: true
        read_only_option: Safe # Only `Safe` or `LeaseBased` is supported
        disable_proposal_forwarding: false
        disable_conf_change_validation: false

    # Meta store configurations for internal use only
    # Both metadata server and metadata client will interact with meta store
    metastore:
        metadata_keep_versions: 5
        data_dirs:
            # Timeplusd doesn't support normal yaml list yet
            dir1: /var/lib/timeplusd/metastore/
        log:
            check_crcs: false
            retention_check_ms: 300000 # Every 5 minutes
            # Fetch related configs
            fetch_max_wait_ms: 500 # max wait time if no records are available
            fetch_max_bytes: 65536 # max bytes to fetch per request
            # Log related configs
            max_entry_size: 10485760
            segment_size: 104857600 # 100MB
            retention_size: 1
            retention_ms: 86400000
            # Minimize log to keep around which is immune to retention policy
            min_size_to_keep: 1
            # For metadata log, we don't actually care the timestamp, sn inverted indexes.
            # So set a big interval bytes and entries
            index_interval_bytes: 4294967296
            index_interval_entries: 100000000
            # We like to flush per message for metadata log
            flush_interval_ms: 120000
            flush_interval_entries: 1
            flush_bytes: 1
            compression_codec: none
            cache_max_cached_entries_per_shard: 100
            cache_max_cached_bytes_per_shard: 4194304
            preallocate: true
            # Raft hard state checkpoint log size, checkpoint log will get rolled
            # and GCed every \hard_state_ckpt_log_size
            hard_state_ckpt_log_size: 1048576 # 1MB
            hard_state_ckpt_log_preallocate: true
            # Raft leader epoch sequence checkpoint log segment size, checkpoint
            # segment will be rolled per every \leader_epoch_index_log_size and
            # GCed automatically
            leader_epoch_index_log_size: 8388608 # 8MB
            leader_epoch_index_log_preallocate: true
            timestamp_index_log_size: 8388608 # 8MB
            timestamp_index_log_preallocate: true
            log_position_index_log_size: 8388608 # 8MB
            log_position_index_log_preallocate: true

data:
    # Only node with `data` role will start data server internally
    # Data server handles internal data request / traffic only
    server:
        # This is internal tcp port for data
        # This is internal listen host / tcp port for metadata
        # It could be different to public listen host
        listen_host: 0.0.0.0
        tcp_port: 8465
        is_port_tls: false
        reuse_socket_address: true
        listen_backlog: 128
        socket_send_buffer_size: 0
        socket_receive_buffer_size: 0

        # connection_max_idle_ms = 0 means no idle timeout management for connections
        connection_max_idle_ms: 120000
        idle_connection_check_interval_ms: 1000

        # Number of network socket server per IP / Port
        # The network thread reads requests from the network
        # and push the request to the request channel / queue
        network_threads: 4

        # Worker threads which picks up the requests from the request queue
        # and do the real work. This is per network socket server.
        # So if the \network_threads = 4 which means 4 network socket server
        # and \worker_threads = 4, then there will be network_threads * worker_threads = 16
        # worker threads in total
        worker_threads: 8

        # Max outstanding queued requests for this host/port
        # when this limit reaches, reject more requests like back pressure
        max_queued_requests: 64

        # interval to log socket client metrics
        metrics_log_interval_ms: 60000

    # All nodes in the cluster will run a meta client since each node
    # will register against with meta (leader) node
    client:
        # Socket related configurations
        max_connections_per_ip: 8
        max_inflight_requests: 10000
        socket_send_buffer_size: 0
        socket_receive_buffer_size: 0
        tcp_keep_alive_probe_sec: 10
        tcp_nodelay: true # Disable Nagle algo
        reconnect_backoff_ms: 500
        reconnect_backoff_max_ms: 6000
        reconnect_max_retries: 0
        dns_refresh_on_fail_interval_sec: 5
        dns_resolution_backoff_ms: 1000
        dns_resolution_backoff_max_ms: 8000
        dns_resolution_max_retries: 0

        # Number of threads to handle server response per client
        response_worker_threads: 4

        # Number of clients to process incoming / outgoing request / response
        # in parallel in a sharded way for all nodes in the cluster
        # Client pool size is usually not greater than the total nodes
        # in the cluster. This client pool is used to internal data replication etc
        client_pool_size: 4

        # client pool size for upper layer application (database layer)
        app_client_pool_size: 4

        # Max stream shards in one fetch session
        fetch_session_max_shards: 1

        # Maximum write buffer size in bytes for one peer node
        # When max write buffer size is full, further write to the socket will fail
        max_send_buffer_size: 536870912 # 512 MB

        # interval to log socket client metrics
        metrics_log_interval_ms: 60000

    raft:
        # \tick_interval_ms is the default resolution of the Raft timer for every tick
        tick_interval_ms: 200
        # \reproposal_timeout_ticks is the number of ticks before
        # reproposing a Raft command
        reproposal_timeout_ticks: 6
        # \raft_scheduler_threads is the default number of Raft scheduler worker threads.
        raft_scheduler_threads: 0
        # \raft_scheduler_shard_threads is the maximum number of Raft scheduler threads per mutex shard.
        # \raft_scheduler_shard_threads shall be small and completely divided by \raft_scheduler_threads.
        raft_scheduler_shard_threads: 0
        # \heartbeat_ticks determine the number of ticks between each heartbeat
        heartbeat_ticks: 2
        # \election_ticks specifies the minimum number of Raft ticks before
        # holding an election. The actual election timeout per shard replica is
        # multiplied by a random factor of 1-2 to avoid ties.
        election_ticks: 4
        # \max_size_per_message specifies the maximum aggregate byte size of Raft
        # log entries that a leader will send to followers in a single AppendMessage
        max_size_per_message: 10485760 # 10 MB
        # \max_committed_size specifies the maximum aggregate byte size of the committed
        # log entries which a node will receive in a single Ready
        max_committed_size: 67108864 # 64 MB
        # \max_uncommitted_entries_size controls how large the uncommitted tail of the
        # Raft log can grow. The limit is meant to provide protection against unbounded
        # Raft log growth when the quorum is lost and entries stop begging committed but
        # continue to be proposed.
        max_uncommitted_entries_size: 134217728 # 128 MB
        # \max_inflight_messages specifies how many `inflight` AppendMessages a leader
        # will send to a given follower without hearing a response.
        max_inflight_messages: 128
        # \max_inflight_bytes specifies how maximum aggregated byte size of Raft log entries
        # that a leader will send to a given follower without hearing a response. As such,
        # it also bounds the amount of replication data buffered in memory on the receiver.
        # Individual message can still exceed this limit (a big log entry for example)
        # Per the bandwidth-delay product, this will limit per shard throughput to
        # 256 MB/s at 500 ms RTT, ~1.28 GB /s at 100 ms RTT, ~12.8 GB/s at 10ms RTT
        max_inflight_bytes: 134217728 # 128 MB
        # \async_storage_writes shall be always true
        async_storage_writes: true
        check_quorum: true
        pre_vote: true
        read_only_option: Safe # Only `Safe` or `LeaseBased` is supported
        disable_proposal_forwarding: false
        disable_conf_change_validation: false

    datastore:
        data_dirs:
            # Timeplusd doesn't support normal yaml list yet
            dir1: /var/lib/timeplusd/nativelog/
        log:
            check_crcs: false
            retention_check_ms: 300000 # Every 5 minutes
            # Fetch related configs
            fetch_max_wait_ms: 500 # max wait time if no records are available
            fetch_max_bytes: 65536 # max bytes to fetch per request
            # Log related configs
            max_entry_size: 10485760
            segment_size: 2147483648 # 2GB
            retention_size: 1
            retention_ms: 86400000
            # Minimize log to keep around which is immune to retention policy
            min_size_to_keep: 2147483648
            index_interval_bytes: 1048576 # 1MB
            index_interval_entries: 1000
            flush_interval_ms: 120000
            flush_interval_entries: 1
            flush_bytes: 1
            compression_codec: none
            cache_max_cached_entries_per_shard: 100
            cache_max_cached_bytes_per_shard: 4194304
            preallocate: true
            # Raft hard state checkpoint log size, checkpoint log will get rolled
            # and GCed every \hard_state_ckpt_log_size
            hard_state_ckpt_log_size: 1048576 # 1MB
            hard_state_ckpt_log_preallocate: true
            # Raft leader epoch sequence checkpoint log segment size, checkpoint
            # segment will be rolled per every \leader_epoch_index_log_size and
            # GCed automatically
            leader_epoch_index_log_size: 8388608 # 8MB
            leader_epoch_index_log_preallocate: true
            timestamp_index_log_size: 8388608 # 8MB
            timestamp_index_log_preallocate: true
            log_position_index_log_size: 8388608 # 8MB
            log_position_index_log_preallocate: true

cluster:
    # This configuration is used to bootstrap the whole cluster.
    # Need specify the full quorum configuration.
    # It is used for internal use only and the internal tcp port is used.
    # It is required to be configured to include the all metadata nodes.
    # Usually the host in the quorum shall be network-addressable by peer
    # meta nodes (advertised_host) globally.
    metadata_node_quorum: localhost:8464

# We support KafkaLog as external log store or NativeLog as the log store.
logstore:
    # Multiple clusters of streaming storage are supported
    kafkalog:
        enabled: false
        default: true # true means, the streaming storage will be used to provision system internal topics
        cluster_name: default-sys-kafka-cluster-name
        cluster_id: default-sys-kafka-cluster-id
        security_protocol: PLAINTEXT # support PLAINTEXT, SASL_SSL
        # if security_protocol is SASL_SSL, username and password should be specified.
        # username:
        # password:
        # Setup the Kafka brokers here
        brokers: localhost:9092
        # Group ID used internally. Once it is setup, can't be changed.
        # Otherwise, there may have data duplication as timeplusd uses it
        # for checkpointing
        group_id: timeplusd
        replication_factor: 1
        # Refer to https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md
        debug: # generic, broker, topic etc
        # Note Redpanda doesn't support idempotence for compact topic
        enable_idempotence: false
        topic_metadata_refresh_interval_ms: 300000
        message_max_bytes: 1000000
        statistic_internal_ms: 30000
        queue_buffering_max_messages: 100000
        queue_buffering_max_kbytes: 1048576
        queue_buffering_max_ms: 50
        message_send_max_retries: 2
        retry_backoff_ms: 100
        compression_codec: snappy
        message_timeout_ms: 40000
        message_delivery_async_poll_ms: 100
        message_delivery_sync_poll_ms: 10
        check_crcs: false
        auto_commit_interval_ms: 5000
        fetch_message_max_bytes: 1048576
        fetch_wait_max_ms: 500
        queued_min_messages: 1000000
        queued_max_messages_kbytes: 65536
        session_timeout_ms: 10000
        max_poll_interval_ms: 30000
        dedicated_subscription_pool_size: 2
        shared_subscription_pool_max_size: 10
        shared_subscription_flush_threshold_count: 10000
        shared_subscription_flush_threshold_size: 10485760
        shared_subscription_flush_threshold_ms: 1000
        streaming_processing_pool_size: 100

# listen_backlog: 64
max_connections: 4096

# For 'Connection: keep-alive' in HTTP 1.1
keep_alive_timeout: 3

# Enable telemetry. This is used to collect the version and runtime environment information to Timeplus, Inc.
telemetry_enabled:
    "@from_env": TELEMETRY_ENABLED
    "@replace": true
    "#text": true

# gRPC protocol (see src/Server/grpc_protos/proton_grpc.proto for the API)
grpc:
    enabled: false
    port: 9100
    is_tls_port: false

    # The following two files are used only if is_tls_port =1
    ssl_cert_file: /path/to/ssl_cert_file
    ssl_key_file: /path/to/ssl_key_file

    # Whether server will request client for a certificate
    ssl_require_client_auth: false

    # The following file is used only if ssl_require_client_auth=1
    ssl_ca_cert_file: /path/to/ssl_ca_cert_file

    # Default compression algorithm (applied if client doesn't specify another algorithm).
    # Supported algorithms: none, deflate, gzip, stream_gzip
    compression: deflate

    # Default compression level (applied if client doesn't specify another level).
    # Supported levels: none, low, medium, high
    compression_level: medium

    # Send/receive message size limits in bytes. -1 means unlimited
    max_send_message_size: -1
    max_receive_message_size: -1

    # Enable if you want very detailed logs
    verbose_logs: false

# Used with https_port and tcp_port_secure. Full ssl options list: https://github.com/timeplusd-Extras/poco/blob/master/NetSSL_OpenSSL/include/Poco/Net/SSLManager.h#L71
openSSL:
    server:
        # Used for https server AND secure tcp port
        # openssl req -subj "/CN=localhost" -new -newkey rsa:2048 -days 365 -nodes -x509 -keyout /etc/timeplusd-server/server.key -out /etc/timeplusd-server/server.crt
        certificateFile: /etc/timeplusd-server/server.crt
        privateKeyFile: /etc/timeplusd-server/server.key

        # dhparams are optional. You can delete the dhParamsFile: element.
        # To generate dhparams, use the following command:
        # openssl dhparam -out /etc/timeplusd-server/dhparam.pem 4096
        # Only file format with BEGIN DH PARAMETERS is supported.
        dhParamsFile: /etc/timeplusd-server/dhparam.pem
        verificationMode: none
        loadDefaultCAFile: true
        cacheSessions: true
        disableProtocols: 'sslv2,sslv3'
        preferServerCiphers: true
    client:
        # Used for connecting to https dictionary source
        loadDefaultCAFile: true
        cacheSessions: true
        disableProtocols: 'sslv2,sslv3'
        preferServerCiphers: true

        # Use for self-signed: verificationMode: none
        invalidCertificateHandler:
            # Use for self-signed: name: AcceptCertificateHandler
            name: RejectCertificateHandler

# Default root page on http[s] server. For example load UI from https://tabix.io/ when opening http://localhost:8123
# http_server_default_response: |-
#     <html ng-app="SMI2"><head><base href="http://ui.tabix.io/"></head><body><div ui-view="" class="content-ui"></div><script src="http://loader.tabix.io/master.js"></script></body></html>

# Maximum memory usage (resident set size) for server process.
# Zero value or unset means default. Default is "max_server_memory_usage_to_ram_ratio" of available physical RAM.
# If the value is larger than "max_server_memory_usage_to_ram_ratio" of available physical RAM, it will be cut down.

# The constraint is checked on query execution time.
# If a query tries to allocate memory and the current memory usage plus allocation is greater
# than specified threshold, exception will be thrown.

# It is not practical to set this constraint to small values like just a few gigabytes,
# because memory allocator will keep this amount of memory in caches and the server will deny service of queries.
max_server_memory_usage: 0

# Maximum number of threads in the Global thread pool.
# This will default to a maximum of 10000 threads if not specified.
# This setting will be useful in scenarios where there are a large number
# of distributed queries that are running concurrently but are idling most
# of the time, in which case a higher number of threads might be required.
max_thread_pool_size: 10000

# On memory constrained environments you may have to set this to value larger than 1.
max_server_memory_usage_to_ram_ratio: 0.8

# Maximum local disk utilization. Once it is hit, all ingest will be errored out
max_local_disk_usage_ratio: 0.9

# Simple server-wide memory profiler. Collect a stack trace at every peak allocation step (in bytes).
# Data will be stored in system.trace_log table with query_id = empty string.
# Zero means disabled.
total_memory_profiler_step: 4194304

# Collect random allocations and deallocations and write them into system.trace_log with 'MemorySample' trace_type.
# The probability is for every alloc/free regardless to the size of the allocation.
# Note that sampling happens only when the amount of untracked memory exceeds the untracked memory limit,
# which is 4 MiB by default but can be lowered if 'total_memory_profiler_step' is lowered.
# You may want to set 'total_memory_profiler_step' to 1 for extra fine grained sampling.
total_memory_tracker_sample_probability: 0

# Limit the size of the cache. e.g., uncompressed_cache_size or mark_cache_size
cache_size_to_ram_max_ratio: 0.5

# Set limit on number of open files (default: maximum). This setting makes sense on Mac OS X because getrlimit() fails to retrieve
# correct maximum value.
# max_open_files: 262144

# Size of cache of uncompressed blocks of data, used in tables of MergeTree family.
# In bytes. Cache is single for server. Memory is allocated only on demand.
# Cache is used when 'use_uncompressed_cache' user setting turned on (off by default).
# Uncompressed cache is advantageous only for very short queries and in rare cases.

# Note: uncompressed cache can be pointless for lz4, because memory bandwidth
# is slower than multi-core decompression on some server configurations.
# Enabling it can sometimes paradoxically make queries slower.
uncompressed_cache_size: 8589934592

# Approximate size of mark cache, used in tables of MergeTree family.
# In bytes. Cache is single for server. Memory is allocated only on demand.
# You should not lower this value.
mark_cache_size: 5368709120

# If you enable the `min_bytes_to_use_mmap_io` setting,
# the data in MergeTree tables can be read with mmap to avoid copying from kernel to userspace.
# It makes sense only for large files and helps only if data reside in page cache.
# To avoid frequent open/mmap/munmap/close calls (which are very expensive due to consequent page faults)
# and to reuse mappings from several threads and queries,
# the cache of mapped files is maintained. Its size is the number of mapped regions (usually equal to the number of mapped files).
# The amount of data in mapped files can be monitored
# in system.metrics, system.metric_log by the MMappedFiles, MMappedFileBytes metrics
# and in system.asynchronous_metrics, system.asynchronous_metrics_log by the MMapCacheCells metric,
# and also in system.events, system.processes, system.query_log, system.query_thread_log, system.query_views_log by the
# CreatedReadBufferMMap, CreatedReadBufferMMapFailed, MMappedFileCacheHits, MMappedFileCacheMisses events.
# Note that the amount of data in mapped files does not consume memory directly and is not accounted
# in query or server memory usage - because this memory can be discarded similar to OS page cache.
# The cache is dropped (the files are closed) automatically on removal of old parts in MergeTree,
# also it can be dropped manually by the SYSTEM DROP MMAP CACHE query.
mmap_cache_size: 1000

# Cache size for compiled expressions.
compiled_expression_cache_size: 134217728

# Policy from the <storage_configuration> for the temporary files.
# If not set <tmp_path> is used, otherwise <tmp_path> is ignored.

# Notes:
# - move_factor              is ignored
# - keep_free_space_bytes    is ignored
# - max_data_part_size_bytes is ignored
# - you must have exactly one volume in that policy
# tmp_policy: tmp

# Directory with user provided files that are accessible by 'file' table function.
user_files_path: /var/lib/timeplusd/user_files/

# LDAP server definitions.
ldap_servers: ''

python_path: /usr/lib/python3.8:/usr/local/lib/python3.8/dist-packages:/usr/lib/python3/dist-packages

# List LDAP servers with their connection parameters here to later 1) use them as authenticators for dedicated local users,
# who have 'ldap' authentication mechanism specified instead of 'password', or to 2) use them as remote user directories.
# Parameters:
# host - LDAP server hostname or IP, this parameter is mandatory and cannot be empty.
# port - LDAP server port, default is 636 if enable_tls is set to true, 389 otherwise.
# bind_dn - template used to construct the DN to bind to.
# The resulting DN will be constructed by replacing all '{user_name}' substrings of the template with the actual
# user name during each authentication attempt.
# user_dn_detection - section with LDAP search parameters for detecting the actual user DN of the bound user.
# This is mainly used in search filters for further role mapping when the server is Active Directory. The
# resulting user DN will be used when replacing '{user_dn}' substrings wherever they are allowed. By default,
# user DN is set equal to bind DN, but once search is performed, it will be updated with to the actual detected
# user DN value.
# base_dn - template used to construct the base DN for the LDAP search.
# The resulting DN will be constructed by replacing all '{user_name}' and '{bind_dn}' substrings
# of the template with the actual user name and bind DN during the LDAP search.
# scope - scope of the LDAP search.
# Accepted values are: 'base', 'one_level', 'children', 'subtree' (the default).
# search_filter - template used to construct the search filter for the LDAP search.
# The resulting filter will be constructed by replacing all '{user_name}', '{bind_dn}', and '{base_dn}'
# substrings of the template with the actual user name, bind DN, and base DN during the LDAP search.
# Note, that the special characters must be escaped properly in XML.
# verification_cooldown - a period of time, in seconds, after a successful bind attempt, during which a user will be assumed
# to be successfully authenticated for all consecutive requests without contacting the LDAP server.
# Specify 0 (the default) to disable caching and force contacting the LDAP server for each authentication request.
# enable_tls - flag to trigger use of secure connection to the LDAP server.
# Specify 'no' for plain text (ldap://) protocol (not recommended).
# Specify 'yes' for LDAP over SSL/TLS (ldaps://) protocol (recommended, the default).
# Specify 'starttls' for legacy StartTLS protocol (plain text (ldap://) protocol, upgraded to TLS).
# tls_minimum_protocol_version - the minimum protocol version of SSL/TLS.
# Accepted values are: 'ssl2', 'ssl3', 'tls1.0', 'tls1.1', 'tls1.2' (the default).
# tls_require_cert - SSL/TLS peer certificate verification behavior.
# Accepted values are: 'never', 'allow', 'try', 'demand' (the default).
# tls_cert_file - path to certificate file.
# tls_key_file - path to certificate key file.
# tls_ca_cert_file - path to CA certificate file.
# tls_ca_cert_dir - path to the directory containing CA certificates.
# tls_cipher_suite - allowed cipher suite (in OpenSSL notation).
# Example:
# my_ldap_server:
#     host: localhost
#     port: 636
#     bind_dn: 'uid={user_name},ou=users,dc=example,dc=com'
#     verification_cooldown: 300
#     enable_tls: yes
#     tls_minimum_protocol_version: tls1.2
#     tls_require_cert: demand
#     tls_cert_file: /path/to/tls_cert_file
#     tls_key_file: /path/to/tls_key_file
#     tls_ca_cert_file: /path/to/tls_ca_cert_file
#     tls_ca_cert_dir: /path/to/tls_ca_cert_dir
#     tls_cipher_suite: ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:AES256-GCM-SHA384

# Example (typical Active Directory with configured user DN detection for further role mapping):
# my_ad_server:
#     host: localhost
#     port: 389
#     bind_dn: 'EXAMPLE\{user_name}'
#     user_dn_detection:
#         base_dn: CN=Users,DC=example,DC=com
#         search_filter: '(&amp;(objectClass=user)(sAMAccountName={user_name}))'
#     enable_tls: no

# To enable Kerberos authentication support for HTTP requests (GSS-SPNEGO), for those users who are explicitly configured
# to authenticate via Kerberos, define a single 'kerberos' section here.
# Parameters:
# principal - canonical service principal name, that will be acquired and used when accepting security contexts.
# This parameter is optional, if omitted, the default principal will be used.
# This parameter cannot be specified together with 'realm' parameter.
# realm - a realm, that will be used to restrict authentication to only those requests whose initiator's realm matches it.
# This parameter is optional, if omitted, no additional filtering by realm will be applied.
# This parameter cannot be specified together with 'principal' parameter.
# Example:
# kerberos: ''

# Example:
# kerberos:
#     principal: HTTP/timeplusd.example.com@EXAMPLE.COM

# Example:
# kerberos:
#     realm: EXAMPLE.COM

# Sources to read users, roles, access rights, profiles of settings, quotas.
user_directories:
    users_xml:
        # Path to configuration file with predefined users.
        path: users.yaml
    local_directory:
        # Path to folder where users created by SQL commands are stored.
        path: /var/lib/timeplusd/access/

#   # To add an LDAP server as a remote user directory of users that are not defined locally, define a single 'ldap' section
#   # with the following parameters:
#   # server - one of LDAP server names defined in 'ldap_servers' config section above.
#   # This parameter is mandatory and cannot be empty.
#   # roles - section with a list of locally defined roles that will be assigned to each user retrieved from the LDAP server.
#   # If no roles are specified here or assigned during role mapping (below), user will not be able to perform any
#   # actions after authentication.
#   # role_mapping - section with LDAP search parameters and mapping rules.
#   # When a user authenticates, while still bound to LDAP, an LDAP search is performed using search_filter and the
#   # name of the logged in user. For each entry found during that search, the value of the specified attribute is
#   # extracted. For each attribute value that has the specified prefix, the prefix is removed, and the rest of the
#   # value becomes the name of a local role defined in timeplusd, which is expected to be created beforehand by
#   # CREATE ROLE command.
#   # There can be multiple 'role_mapping' sections defined inside the same 'ldap' section. All of them will be
#   # applied.
#   # base_dn - template used to construct the base DN for the LDAP search.
#   # The resulting DN will be constructed by replacing all '{user_name}', '{bind_dn}', and '{user_dn}'
#   # substrings of the template with the actual user name, bind DN, and user DN during each LDAP search.
#   # scope - scope of the LDAP search.
#   # Accepted values are: 'base', 'one_level', 'children', 'subtree' (the default).
#   # search_filter - template used to construct the search filter for the LDAP search.
#   # The resulting filter will be constructed by replacing all '{user_name}', '{bind_dn}', '{user_dn}', and
#   # '{base_dn}' substrings of the template with the actual user name, bind DN, user DN, and base DN during
#   # each LDAP search.
#   # Note, that the special characters must be escaped properly in XML.
#   # attribute - attribute name whose values will be returned by the LDAP search. 'cn', by default.
#   # prefix - prefix, that will be expected to be in front of each string in the original list of strings returned by
#   # the LDAP search. Prefix will be removed from the original strings and resulting strings will be treated
#   # as local role names. Empty, by default.
#   # Example:
#   # ldap:
#   #     server: my_ldap_server
#   #     roles:
#   #         my_local_role1: ''
#   #         my_local_role2: ''
#   #     role_mapping:
#   #         base_dn: 'ou=groups,dc=example,dc=com'
#   #         scope: subtree
#   #         search_filter: '(&amp;(objectClass=groupOfNames)(member={bind_dn}))'
#   #         attribute: cn
#   #         prefix: timeplusd_
#   # Example (typical Active Directory with role mapping that relies on the detected user DN):
#   # ldap:
#   #     server: my_ad_server
#   #     role_mapping:
#   #         base_dn: 'CN=Users,DC=example,DC=com'
#   #         attribute: CN
#   #         scope: subtree
#   #         search_filter: '(&amp;(objectClass=group)(member={user_dn}))'
#   #         prefix: timeplusd_

access_control_improvements:
  users_without_row_policies_can_read_rows: false
  # Enables logic that users without permissive row policies can still read rows using a SELECT query.
  # For example, if there two users A, B and a row policy is defined only for A, then
  # if this setting is true the user B will see all rows, and if this setting is false the user B will see no rows.
  # By default this setting is false for compatibility with earlier access configurations.

# Default profile of settings.
default_profile: default

# Comma-separated list of prefixes for user-defined settings.
# custom_settings_prefixes: ''
# System profile of settings. This settings are used by internal processes (Distributed DDL worker and so on).
# system_profile: default

# Buffer profile of settings.
# This settings are used by Buffer storage to flush data to the underlying table.
# Default: used from system_profile directive.
# buffer_profile: default

# Default database.
default_database: default

# Neutron database
neutron_database: neutron

# Server time zone could be set here.

# Time zone is used when converting between String and DateTime types,
# when printing DateTime in text formats and parsing DateTime from text,
# it is used in date and time related functions, if specific time zone was not passed as an argument.

# Time zone is specified as identifier from IANA time zone database, like UTC or Africa/Abidjan.
# If not specified, system time zone at server startup is used.

# Please note, that server could display time zone alias instead of specified name.
# Example: W-SU is an alias for Europe/Moscow and Zulu is an alias for UTC.
# timezone: Europe/Moscow

# You can specify umask here (see "man umask"). Server will apply it on startup.
# Number is always parsed as octal. Default umask is 027 (other users cannot read logs, data files, etc; group can only read).
# umask: 022

# Perform mlockall after startup to lower first queries latency
# and to prevent timeplusd executable from being paged out under high IO load.
# Enabling this option is recommended but will lead to increased startup time for up to a few seconds.
mlock_executable: true

# Reallocate memory for machine code ("text") using huge pages. Highly experimental.
remap_executable: false

# Reloading interval for embedded dictionaries, in seconds. Default: 3600.
builtin_dictionaries_reload_interval: 3600

# Maximum session timeout, in seconds. Default: 3600.
max_session_timeout: 3600

# Default session timeout, in seconds. Default: 60.
default_session_timeout: 60

# Sending data to Graphite for monitoring. Several sections can be defined.
# interval - send every X second
# root_path - prefix for keys
# hostname_in_path - append hostname to root_path (default = true)
# metrics - send data from table system.metrics
# events - send data from table system.events
# asynchronous_metrics - send data from table system.asynchronous_metrics

# graphite:
#     host: localhost
#     port: 42000
#     timeout: 0.1
#     interval: 60
#     root_path: one_min
#     hostname_in_path: true

#     metrics: true
#     events: true
#     events_cumulative: false
#     asynchronous_metrics: true

# graphite:
#     host: localhost
#     port: 42000
#     timeout: 0.1
#     interval: 1
#     root_path: one_sec

#     metrics: true
#     events: true
#     events_cumulative: false
#     asynchronous_metrics: false

# Serve endpoint for Prometheus monitoring.
# endpoint - mertics path (relative to root, statring with "/")
# port - port to setup server. If not defined or 0 than http_port used
# metrics - send data from table system.metrics
# events - send data from table system.events
# asynchronous_metrics - send data from table system.asynchronous_metrics
# status_info - send data from different component from CH, ex: Dictionaries status

prometheus:
    endpoint: /metrics
    port: 9363
    metrics: true
    events: true
    asynchronous_metrics: true
    status_info: true
    external_stream: true
    materialized_view: true
    query_info: true
    cluster: true

# Query log. Used only for queries with setting log_queries = 1.
query_log:
    # What table to insert data. If table is not exist, it will be created.
    # When query log structure is changed after system update,
    # then old table will be renamed and new table will be created automatically.
    database: system
    table: query_log

    # PARTITION BY expr:
    # Example:
    # event_date
    # toMonday(event_date)
    # toYYYYMM(event_date)
    # toStartOfHour(event_time)
    partition_by: to_YYYYMM(event_date)

    # Table TTL specification:
    # Example:
    # event_date + INTERVAL 1 WEEK
    # event_date + INTERVAL 7 DAY DELETE
    # event_date + INTERVAL 2 WEEK TO DISK 'bbb'

    ttl: 'event_date + INTERVAL 30 DAY DELETE'

    # Instead of partition_by, you can provide full engine expression (starting with ENGINE = ) with parameters,
    # Example: engine: 'ENGINE = MergeTree PARTITION BY toYYYYMM(event_date) ORDER BY (event_date, event_time) SETTINGS index_granularity = 1024'

    # Interval of flushing data.
    flush_interval_milliseconds: 7500

# Trace log. Stores stack traces collected by query profilers.
# See query_profiler_real_time_period_ns and query_profiler_cpu_time_period_ns settings.
trace_log:
    database: system
    table: trace_log
    partition_by: to_YYYYMM(event_date)
    flush_interval_milliseconds: 7500
    ttl: 'event_date + INTERVAL 7 DAY DELETE'

# Query thread log. Has information about all threads participated in query execution.
# Used only for queries with setting log_query_threads = 1.
query_thread_log:
    database: system
    table: query_thread_log
    partition_by: to_YYYYMM(event_date)
    ttl: 'event_date + INTERVAL 30 DAY DELETE'
    flush_interval_milliseconds: 7500

# Query views log. Has information about all dependent views associated with a query.
# Used only for queries with setting log_query_views = 1.
query_views_log:
    database: system
    table: query_views_log
    partition_by: to_YYYYMM(event_date)
    flush_interval_milliseconds: 7500
    ttl: 'event_date + INTERVAL 30 DAY DELETE'

# Uncomment if use part log.
# Part log contains information about all actions with parts in MergeTree tables (creation, deletion, merges, downloads).
part_log:
    database: system
    table: part_log
    partition_by: to_YYYYMM(event_date)
    flush_interval_milliseconds: 7500
    ttl: 'event_date + INTERVAL 30 DAY DELETE'

# Uncomment to write text log into table.
# Text log contains all information from usual server log but stores it in structured and efficient way.
# The level of the messages that goes to the table can be limited (<level>), if not specified all messages will go to the table.
# text_log:
#     database: system
#     table: text_log
#     flush_interval_milliseconds: 7500
#     level: ''

# Metric log contains rows with current values of ProfileEvents, CurrentMetrics collected with "collect_interval_milliseconds" interval.
metric_log:
    database: system
    table: metric_log
    flush_interval_milliseconds: 7500
    collect_interval_milliseconds: 1000
    ttl: 'event_date + INTERVAL 30 DAY DELETE'

# Asynchronous metric log contains values of metrics from
# system.asynchronous_metrics.
asynchronous_metric_log:
    database: system
    table: asynchronous_metric_log

    # Asynchronous metrics are updated once a minute, so there is
    # no need to flush more often.
    flush_interval_milliseconds: 60000
    ttl: 'event_date + INTERVAL 7 DAY DELETE'

# OpenTelemetry log contains OpenTelemetry trace spans.
opentelemetry_span_log:

    # The default table creation code is insufficient, this <engine> spec
    # is a workaround. There is no 'event_time' for this log, but two times,
    # start and finish. It is sorted by finish time, to avoid inserting
    # data too far away in the past (probably we can sometimes insert a span
    # that is seconds earlier than the last span in the table, due to a race
    # between several spans inserted in parallel). This gives the spans a
    # global order that we can use to e.g. retry insertion into some external
    # system.
    engine: |-
        engine MergeTree
             partition by to_YYYYMM(finish_date)
             order by (finish_date, finish_time_us, trace_id)
             ttl finish_date + INTERVAL 7 DAY DELETE
    database: system
    table: opentelemetry_span_log
    flush_interval_milliseconds: 7500

# Crash log. Stores stack traces for fatal errors.
# This table is normally empty.
crash_log:
    database: system
    table: crash_log
    partition_by: ''
    flush_interval_milliseconds: 1000

# Processor profile log.
processors_profile_log:
    database: system
    table: processors_profile_log
    partition_by: to_YYYYMM(event_date)
    flush_interval_milliseconds: 7500

stream_metric_log:
    engine: |-
        engine Stream(1, 1, rand())
             order by (to_hour(_tp_time), database, name)
             ttl to_datetime(_tp_time) + INTERVAL 1 YEAR DELETE
        settings local=true
    database: system
    table: stream_metric_log
    flush_interval_milliseconds: 30000   # 30 seconds
    collect_interval_milliseconds: 300000  # 5 minutes

stream_state_log:
    engine: |-
        engine Stream(1, 1, rand())
             order by (to_hour(_tp_time), database, name)
             ttl to_datetime(_tp_time) + INTERVAL 1 MONTH DELETE
        settings local=true
    database: system
    table: stream_state_log
    flush_interval_milliseconds: 30000   # 30 seconds
    collect_interval_milliseconds: 300000  # 5 minutes

# Path to file with region hierarchy.
# path_to_regions_hierarchy_file: /opt/geo/regions_hierarchy.txt

# Path to directory with files containing names of regions
# path_to_regions_names_files: /opt/geo/


# top_level_domains_path: /var/lib/timeplusd/top_level_domains/
# Custom TLD lists.
# Format: name: /path/to/file

# Changes will not be applied w/o server restart.
# Path to the list is under top_level_domains_path (see above).
top_level_domains_lists: ''

# public_suffix_list: /path/to/public_suffix_list.dat

# Configuration of external dictionaries. See:
dictionaries_config: '*_dictionary.xml'

# Uncomment if you want data to be compressed 30-100% better.
# Don't do that if you just started using timeplusd.

# compression:
#     # Set of variants. Checked in order. Last matching case wins. If nothing matches, lz4 will be used.
#     case:
#         Conditions. All must be satisfied. Some conditions may be omitted.
#         # min_part_size: 10000000000    # Min part size in bytes.
#         # min_part_size_ratio: 0.01     # Min size of part relative to whole table size.
#         # What compression method to use.
#         method: zstd

# Settings to fine tune MergeTree tables. See documentation in source code, in MergeTreeSettings.h
# merge_tree:
#     max_suspicious_broken_parts: 5

# Protection from accidental DROP.
# If size of a MergeTree table is greater than max_stream_size_to_drop (in bytes) than table could not be dropped with any DROP query.
# If you want do delete one table and don't want to change timeplusd-server config, you could create special file <timeplusd-path>/flags/force_drop_table and make DROP once.
# By default max_stream_size_to_drop is 50GB; max_stream_size_to_drop=0 allows to DROP any tables.
# The same for max_partition_size_to_drop.
# Uncomment to disable protection.

# max_stream_size_to_drop: 0
# max_partition_size_to_drop: 0

# Example of parameters for GraphiteMergeTree table engine
graphite_rollup_example:
    pattern:
        regexp: click_cost
        function: any
        retention:
            - age: 0
              precision: 3600
            - age: 86400
              precision: 60
    default:
        function: max
        retention:
            - age: 0
              precision: 60
            - age: 3600
              precision: 300
            - age: 86400
              precision: 3600

# Directory in <timeplusd-path> containing schema files for various input formats.
# The directory will be created if it doesn't exist.
format_schema_path: /var/lib/timeplusd/format_schemas/

# Default query masking rules, matching lines would be replaced with something else in the logs
# (both text logs and system.query_log).
# name - name for the rule (optional)
# regexp - RE2 compatible regular expression (mandatory)
# replace - substitution string for sensitive data (optional, by default - six asterisks)
query_masking_rules:
    rule:
        name: hide encrypt/decrypt arguments
        regexp: '((?:aes_)?(?:encrypt|decrypt)(?:_mysql)?)\s*\(\s*(?:''(?:\\''|.)+''|.*?)\s*\)'
        # or more secure, but also more invasive:
        # (aes_\w+)\s*\(.*\)
        replace: \1(???)

# Uncomment to use custom http handlers.
# rules are checked from top to bottom, first match runs the handler
# url - to match request URL, you can use 'regex:' prefix to use regex match(optional)
# methods - to match request method, you can use commas to separate multiple method matches(optional)
# headers - to match request headers, match each child element(child element name is header name), you can use 'regex:' prefix to use regex match(optional)
# handler is request handler
# type - supported types: static, dynamic_query_handler, predefined_query_handler
# query - use with predefined_query_handler type, executes query when the handler is called
# query_param_name - use with dynamic_query_handler type, extracts and executes the value corresponding to the <query_param_name> value in HTTP request params
# status - use with static type, response status code
# content_type - use with static type, response content-type
# response_content - use with static type, Response content sent to client, when using the prefix 'file://' or 'config://', find the content from the file or configuration send to client.

# NOTE, if enable these dynamic handlers, please follow /timeplusd/{uri_path} convention, check createHandlersFactoryFromConfig() in HTTPHandlerFactory.cpp
# http_handlers:
#     - rule:
#         url: /
#         methods: POST,GET
#         headers:
#           pragma: no-cache
#         handler:
#           type: dynamic_query_handler
#           query_param_name: query
#     - rule:
#         url: /predefined_query
#         methods: POST,GET
#         handler:
#           type: predefined_query_handler
#           query: 'SELECT * FROM system.settings'
#     - rule:
#         handler:
#           type: static
#           status: 200
#           content_type: 'text/plain; charset=UTF-8'
#           response_content: config://http_server_default_response

send_crash_reports:
    # Changing <enabled> to true allows sending crash reports to
    # the timeplusd core developers team via Sentry https://sentry.io
    # Doing so at least in pre-production environments is highly appreciated
    enabled: false
    # Change <anonymize> to true if you don't feel comfortable attaching the server hostname to the crash report
    anonymize: false
    # Default endpoint should be changed to different Sentry DSN only if you have
    # some in-house engineers or hired consultants who're going to debug timeplsud issues for you
    endpoint:

# Uncomment to disable timeplsud internal DNS caching.
# disable_internal_dns_cache: 1

#rocksdb:
#    options:
#        max_background_jobs: 8
#    column_family_options:
#        num_levels: 2
#    tables:
#        - table
#            name: TABLE
#            options:
#                max_background_jobs: 8
#            column_family_options:
#                num_levels: 2

# User defined grok patterns file
grok_patterns_file: /etc/timeplusd-server/grok-patterns

# Maximum number of concurrent queries.
max_concurrent_queries: 100
# Maximum number of concurrent insert queries.
max_concurrent_insert_queries: 100
# Maximum number of concurrent select queries.
max_concurrent_select_queries: 100

# On Linux systems this can control the behavior of OOM killer.
# oom_score: 1000

settings:
    global:
        query_mode: streaming               # Default query mode. table or streaming
        query_resource_group: "dedicated"   # Default resource group. dedicated or shared
        enable_light_ingest: true           # Light ingest is inserting partial columns of a table
        _tp_enable_log_stream_expr: true    # Log system analytic
        synchronous_ddl: true               # If setting is enabled, the DDL for streaming storage will be executed synchronously otherwise it will be asynchronous. By default is enabled.
        asterisk_include_reserved_columns: true # Show reserved columns on SELECT query.
        async_ingest_block_timeout_ms: 12000    # Max duration for a block to commit before it is considered expired during async ingestion
        aysnc_ingest_max_outstanding_blocks: 10000 # Max outstanding blocks to be committed per stream during async ingestion
        part_commit_pool_size: 8 # Total shared thread pool size for building and committing parts for Stream
        max_idempotent_ids: 1000 # Maximum idempotent IDs to keep in memory and on disk for idempotent data ingestion
        _tp_internal_system_open_sesame: true # Control the access to system.* streams

        javascript_max_memory_bytes: 204857600 # Maximum heap size of javascript UDA/UDF in bytes, default is 200*1024*1024 bytes
        recovery_policy: "best_effort" # Recovery policy for materialized view. strict or best_effort
        recovery_retry_for_sn_failure: 3 # retry times for sn failure. this value only apply if the `recovery_policy` is `best_effort`
        mv_election_timeout_factor_for_best_candidates: 0.3       # "Adjusts election timeout for nodes in desired state
        mv_election_timeout_factor_for_fallback_candidates: 1.5      # "Adjusts election timeout for peer nodes acting as fallbacks
        mv_election_timeout_factor_for_overloaded_candidates: 2.0    # "Adjusts election timeout for nodes experiencing overload

        max_block_size: 65409 # 65536 - (PADDING_FOR_SIMD - 1)
        max_insert_block_size: 65409 # 65536 - (PADDING_FOR_SIMD - 1)
        nlog_adhoc_pool_size: 16 # Thread pool size for nativelog ad-hoc tasks

    stream:
        default_shards: 1
        default_replication_factor: 1
        default_sharding_expr: ""   # Empty string means `rand()` or `weak_hash32(<primary_keys>) if primary key set`
        ingest_mode: "sync"    # Data ingestion mode for Stream
        logstore: "nativelog"
        logstore_replication_factor: 1
        storage_type: "hybrid"
        logstore_codec: none
        logstore_retention_bytes: 0 # when this threshold reaches, streaming storage delete old data. -1 means no limit
        logstore_retention_ms: 0 # when this threshold reaches, streaming storage delete old data. -1 means no limit

# key-value service for external use, usually for neutron
kv_service:
    enabled: false
    http_port: 9444
    server_id: 1

    namespace_whitelist:
        namespace:
            - "proton"
            - "neutron"
            - "udf"

    coordination_settings:
        # <!-- Default client session timeout -->
        session_timeout_ms: 30000

        # <!-- Default client operation timeout -->
        operation_timeout_ms: 10000

        # <!-- Heartbeat interval between quorum nodes, Default:500 -->
        # <!-- <heart_beat_interval_ms>500</heart_beat_interval_ms> -->

        # <!-- Lower bound of election timer (avoid too often leader elections), Default: 1000 -->
        # <!-- <election_timeout_lower_bound_ms>1000</election_timeout_lower_bound_ms> -->

        # <!-- Upper bound of election timer (avoid too often leader elections), Default: 2000 -->
        # <!-- <election_timeout_upper_bound_ms>2000</election_timeout_upper_bound_ms> -->

        # <!-- Log internal RAFT logs into main server log level. Default: information
        #         Valid values: 'trace', 'debug', 'information', 'warning', 'error', 'fatal', 'none'
        # -->
        raft_logs_level: information

        # <!-- Call fsync on each change in RAFT changelog, Default: true -->
        force_sync: false

        # <!-- How many time we will until RAFT to start, Default: 30000 -->
        startup_timeout: 60000

        # <!-- How many time we will until RAFT shutdown, Default: 5000 -->
        shutdown_timeout: 5000

        # <!-- When node became stale and should receive snapshots from leader, Default: 10000 -->
        stale_log_gap: 10000

        # <!-- When node became fresh, Default: 200 -->
        fresh_log_gap: 200

        # <!-- How many log items to store (don't remove during compaction), Default: 100000 -->
        # <!-- we want all logs for complex problems investigation -->
        reserved_log_items: 100000

        # <!-- How many log items we have to collect to write new snapshot, Default: 100000 -->
        snapshot_distance: 100000

        # <!-- Max size of batch in requests count before it will be sent to RAFT, Default: 100 -->
        max_requests_batch_size: 100

        # <!-- Allow to forward write requests from followers to leader, Default: true -->
        auto_forwarding: true

        # <!-- Execute read requests as writes through whole RAFT consesus with similar speed, Default: false -->
        quorum_reads: false

    raft_configuration:
        server:
            id: 1
            hostname: localhost
            port: 9445
            http_port: 3218
